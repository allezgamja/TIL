# RNN

현재의 입력데이터 xt를 처리할 때 입력으로 xt와 이전의 출력인 h(t-1)을 둘다 받는다. 즉, 이전에 학습된 정보를 다음 시점에 넣어서 새로 들어오는 정보와 "함께" 학습시킨다. RNN에서는 뉴런과 층을 셀(cell)이라고 하며, 출력은 은닉 상태(hidden state)라고 한다.



#### NN과 RNN의 차이점

NN: sigmoid(Wx * xt + b)
RNN: tanh(Wx * xt + Wh * h(t-1) + b

- 인공신경망(NN)은 시간의 흐름을 반영하지 않으나, RNN은 <u>기존에 학습했던 정보를 다시 입력한다.
- 따라서 시간의 흐름(sequence), 즉 timestep을 반영.</u> 시간차 행을 각각의 독립관계로 보지 않으며, 시간 데이터 간의 상관관계를 볼 수 있다.



#### RNN의 특징

- RNN은 셀의 타입스탭을 몇 시점까지 봐야되느냐가 중요하다. 어느 과거 시점까지 반영을 해야하는지가 NN에서의 층의 개념과 같다.
- 대표적으로는 텍스트데이터를 분석하기 위해 필요하다. 문맥은 단어끼리 서로 연관돼있으므로.
  - e.g. "나는 밥을 먹었다."  -> '먹었다'라는 말 앞에는 주로 어떤 단어가 오는가? '나는'을 먼저 넣고 '밥을'을 나중에 넣는다.

- 입력과 출력의 개수를 마음대로 조정할 수 있다. 
  - synched many to many: 입력과 중간층의 손실이 없이 출력
  - one to many: 이미지를 하나 넣고 captioning . e.g. 이미지 넣으면 쇼핑몰 상품정보 생성되는 서비스
  - many to one: e.g. 여러 개의 단어를 집어넣고 감정분석
  - many to many; e.g. 번역

- tanh의 범위는 -1에서 1 사이





# LSTM

LSTM은 RNN의 문제점인 gradient vanishing을 보완하기 위해 등장.



gradient vanishing이란?

층이 깊어지는 만큼 액티베이션 함수가 결과값에 자주 들어가게 된다. 이때 function의 일정 수가 넘어버리면 역전파를 이용할 때 기울기가 0으로 수렴하여 더 이상 내려가지 않는다. 중간에 0이 되어버리면 w값에 얼만큼의 오차가 있는지 알 수 없게 되므로, tanh대신 relu 등의 발산하는 식을 사용해야 한다.



LSTM은 과거에서 받은 데이터를 중요한 값과 그렇지 않은 값들로 control한다. 과거데이터는 곱하기(x), 현재데이터는 더하기(+). 과거 데이터 중 중요한 값들은 가중치를 크게 증폭하고, 의미 없는 값들은 자체적으로 줄인다.